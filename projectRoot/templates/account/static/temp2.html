{% load static %}

<title>Document</title>
<script src="{% static "face-api.min.js" %}"></script>
<style> body {
    margin: 0;
    padding: 0;
    width: 100vw;
    height: 100vh;
    display: flex;
    justify-content: center;
    align-items: center;
}

canvas {
    position: absolute;
} </style>
<script type="text/javascript">
    const video = document.getElementById('video')
    Promise.all([
        faceapi.nets.tinyFaceDetector.loadFromUri('{% static '/models' %}'),
        faceapi.nets.faceLandmark68Net.loadFromUri('{% static '/models' %}'),
        faceapi.nets.faceRecognitionNet.loadFromUri('{% static '/models' %}'),
        faceapi.nets.faceExpressionNet.loadFromUri('{% static '/models' %}'),
        faceapi.nets.ageGenderNet.loadFromUri('{% static '/models' %}')]
    ).then(startVideo)

    function startVideo() {
        navigator.getUserMedia({video: {}}, stream => video.srcObject = stream, err => console.error(err))
    }

    video.addEventListener('play', () => {
        const canvas = faceapi.createCanvasFromMedia(video)
        document.body.append(canvas)
        const displaySize = {width: video.width, height: video.height}
        faceapi.matchDimensions(canvas, displaySize)
        setInterval(async () => {
            const detections = await faceapi.detectAllFaces(video, new faceapi.TinyFaceDetectorOptions()).withFaceLandmarks().withFaceExpressions()
            const resizedDetections = faceapi.resizeResults(detections, displaySize)
            canvas.getContext('2d').clearRect(0, 0, canvas.width, canvas.height)
            faceapi.draw.drawDetections(canvas, resizedDetections)
            faceapi.draw.drawFaceLandmarks(canvas, resizedDetections)
            faceapi.draw.drawFaceExpressions(canvas, resizedDetections)
        }, 100)
    }) </script>